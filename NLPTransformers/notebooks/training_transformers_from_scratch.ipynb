{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f47a4e-2a96-49e3-9ffd-8c6039b38014",
   "metadata": {},
   "source": [
    "Comparison text generations from GPT and GPT-2 to illustrate the notion of a model being skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1d6f10-78a5-4c7b-a497-74955e692ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb86ab156e54f70a39758ff7ff46128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97242be676894e0997d4827abca76652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2025-07-01T14:46:16.894739Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, url: \\\"https://cas-server.xethub.hf.co/reconstruction/986657f8b77a4a690d73500aa22cbda25b96a55bd9b689e8312ebd983a7108d5\\\", source: hyper_util::client::legacy::Error(Connect, ConnectError(\\\"dns error\\\", Custom { kind: Uncategorized, error: \\\"failed to lookup address information: Name or service not known\\\" })) }). Retrying...\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":200}\n",
      "{\"timestamp\":\"2025-07-01T14:46:16.894830Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 651.203428ms before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533b818b7e64432fa99426bf02161474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b30f58e0ffd4bee887a34445692bc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa0e60b62aa4cceaea897505b7685b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be28df425204cf6b08c177c25bb9062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9c958da2264b1bac1f6430eed703c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
    "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9918d9-ebe3-4350-8779-5e98a0472a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708da9a0-eaed-4585-9f24-49f74fe1d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"GPT size:{model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
    "print(f\"GPT2 size:{model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a33f24-0706-4e38-b4d2-a6ff734693de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT completions:\n",
      "1.\n",
      "When they came back from the cemetery and said their good - byes at the gravesite. \n",
      " at first i was so happy to have them return to me, i couldn't contain myself. with the relief we could share and the sense of relief they\n",
      "2.\n",
      "When they came back into the room. they were smiling at me and i thought i saw a hint of relief. we were all exhausted as we headed down the hallway and into the living room. \n",
      " \" where have you been? we've been so\n",
      "3.\n",
      "When they came back out to the clearing she was with, but they didn't see her. when she stopped by the river to drink from the spring, she saw a strange object floating across toward her. then, as suddenly as it had appeared,\n",
      "\n",
      "GPT2 completions:\n",
      "1.\n",
      "When they came back, there was a small group of us waiting for them there as our security guard stood by them, then they began to run, kicking, slashing and trying to frighten me into silence. It took about ten minutes before they\n",
      "2.\n",
      "When they came back, the car was already moving.\n",
      "\n",
      "\"There was a guy who was wearing a hooded sweatshirt that was holding a stick at the foot of his chest,\" says a relative who knew the man, calling himself Robert\n",
      "3.\n",
      "When they came back for it, they decided not to try it out because you need to be careful with the weight (and you will burn lots of fuel).\n",
      "\n",
      "\n",
      "We are now back in business!! We think of the next thing we need\n"
     ]
    }
   ],
   "source": [
    "def enum_pipeline_outputs(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(prompt, num_return_sequences=num_return_sequences, clean_up_tokenization_spaces=True)\n",
    "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
    "\n",
    "prompt = \"\\nWhen they came back\"\n",
    "print(\"GPT completions:\\n\" + enum_pipeline_outputs(generation_gpt, prompt, 3))\n",
    "print(\"\")\n",
    "print(\"GPT2 completions:\\n\" + enum_pipeline_outputs(generation_gpt2, prompt, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d4449-a5a1-4640-b70e-5910a195b10c",
   "metadata": {},
   "source": [
    "## Building a Custom Code Dataset\n",
    "### Creating a dataset with Google BigQuery\n",
    "\n",
    "tota la perafernalia que he hagut de fer per executar una consulta, guardar els resultats en una taula d'un dataset i passar les dades a una carpeta del bucket 'npl_transformers'.\n",
    "Després he instalat el [gcloud CLI](https://cloud.google.com/sdk/docs/install) al servidor.\n",
    "Llavors he pogut llançar la comana: ``gsutil -m -o \"GSUtil:parellel_process_count=1\" cp -r gs:<nom_del_bucket>/<carpeta>/resultados-*.json.gz /datasets/codeparrot``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7403ba6-471b-4174-9674-929dcee53163",
   "metadata": {},
   "source": [
    "## Working with Large Datasets\n",
    "\n",
    "Loading a very large dataset is often a challenging task, in particular when the data is larger than your machine's RAM.\n",
    "In our example, we have 50 GB of compressed data and about 200 GB of uncompressed data.\n",
    "\n",
    "Thankfully Datasets has two specific features that allow you to set yourself free from RAM and hard drive space limitations: **memory mapping** and **streaming**.\n",
    "\n",
    "### Memory mapping\n",
    "\n",
    "Overcoming RAM limitations --> uses a mechanism of zero-copy and zero-overhead memory mapping.\n",
    "Basically, each dataset is cached on the drive in a file that is a direct reflection of the content in RAM memory. Instead of loading the dataset in RAM, it opens a read-only pointer to this file and uses it as a substitue for RAM (using hard drive as an extension of RAM).\n",
    "\n",
    "Here, we will direcly load our 50GB of compressed JSON files that we have stored locally in the *codeparrot* repository.\n",
    "Decompress JSON files. Be careful, cause this uses **180GB of free disk!!!**\n",
    "\n",
    "*delete_extracted=True* in the dataset's downloading configuration, we can make sure that we delete all the files we don't need anymore as soon as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3cf18bd-102f-4341-af46-cc2190a65d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf4fe40d09c4d3d9e7f24d261c7e8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a2c79b4abb469db2f0b77536aa196a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/500 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2370ad6485a54ccfb34c8a6ab90c60fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062f580d205b4fe283a501090776c5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DownloadConfig, Features, Value\n",
    "import os\n",
    "\n",
    "download_config = DownloadConfig(delete_extracted=True)\n",
    "dataset_path = os.path.expanduser(\"~/datasets/codeparrot/export_results/*.json.gz\")\n",
    "\n",
    "features = Features({\n",
    "    \"repo_name\": Value(\"string\"),\n",
    "    \"path\": Value(\"string\"),\n",
    "    \"copies\": Value(\"string\"),\n",
    "    \"size\": Value(\"string\"),\n",
    "    \"content\": Value(\"string\"),\n",
    "    \"license\": Value(\"string\")\n",
    "})\n",
    "\n",
    "dataset = load_dataset(\"json\",\n",
    "                       data_files=dataset_path, split=\"train\",\n",
    "                      download_config=download_config, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4e59bb-79f9-40e0-aa84-8b5bfc6f837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gunzip -c ~/datasets/codeparrot/export_results/resultados-000000000000.json.gz > ./uno.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd876c86-9b6f-4b32-adca-ad959d6110c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d1f510e39e429bac1007878423b674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
      "    num_rows: 38184\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#from datasets import load_dataset\n",
    "#\n",
    "#dataset = load_dataset(\"json\", data_files=\"./uno.json\", split=\"train\")\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94616d64-217a-44d7-8bf9-2861e7a77d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of python files code in dataset : 18923569\n",
      "Dataset size (cache file) : 97.06 GB\n",
      "RAM used: 1720 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"Number of python files code in dataset : {len(dataset)}\")\n",
    "ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
    "# os.stat.st_size is expressed in bytes, so we convert to GB\n",
    "print(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\n",
    "# Process.memory_ingo is expressed in bytes, so we convert to MB\n",
    "print(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b19efb-3b4b-4223-9baa-5d80d233e32f",
   "metadata": {},
   "source": [
    "What if you can't free enough disk space to store the full dataset locally?\n",
    "\n",
    "## Streaming\n",
    "\n",
    "An alternative to scaling up the server you are using is to *stream* the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1894aab4-4113-44ec-89b4-24dcd77274ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa73b0bbc5a440a497246a3f5d008b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "streamed_dataset = load_dataset(\"json\",\n",
    "                       data_files=dataset_path, split=\"train\",\n",
    "                      download_config=download_config, features=features,\n",
    "                               streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9d223-0de4-4bd2-bbf2-a7ee7e495188",
   "metadata": {},
   "source": [
    "In streaming mode, the compressed JSON files will be opened and read on the fly. Out dataset is now an *IterableDataset* object. This means that we cannot access random elements of it (streamed_dataset[1264]), but we need to read it in order, for instance with ``next(iter(streamed_dataset))``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c44e734d-04e7-4d90-b579-cc323fe65daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(streamed_dataset)\n",
    "\n",
    "print(dataset[0] == next(iterator))\n",
    "print(dataset[1] == next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed7e7a-81c1-4408-91e1-eba7e74fb36e",
   "metadata": {},
   "source": [
    "The original raw files are extracted and read on the fly when a new batch of examples is requested, and onñy that batch is loaded in memory.\n",
    "\n",
    "One step further-- instead of pointing to the local dataset we can reference the dataset on the Hub, and then directly download samples without downloading the raw files locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0145c66d-df0c-40bb-8adc-197e1a71126a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9a3927804842c2bcd0a342d438bc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba046b8708a47cea9c872f3e28f2e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remote_dataset = load_dataset('transformersbook/codeparrot', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f38625-c75e-4387-88c7-d0449548fc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb1a0ed048f43a08ac959a902e45ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658476d6-9ac8-4978-b7ae-98f3503bcb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe --type argument is deprecated and will be removed in a future version. Use --repo-type instead.\u001b[0m\n",
      "\u001b[33mThe --organization argument is deprecated and will be removed in a future version. Pass the organization namespace directly in the repo_id.\u001b[0m\n",
      "Successfully created \u001b[1msergi24sanchez/codeparrot-train\u001b[0m on the Hub.\n",
      "Your repo is now available at \u001b[1mhttps://huggingface.co/datasets/sergi24sanchez/codeparrot-train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli repo create --repo-type dataset --organization sergi24sanchez codeparrot-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf855d0-e4c0-403b-85a9-d6f2cf17d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe --organization argument is deprecated and will be removed in a future version. Pass the organization namespace directly in the repo_id.\u001b[0m\n",
      "Successfully created \u001b[1msergi24sanchez/codeparrot-valid\u001b[0m on the Hub.\n",
      "Your repo is now available at \u001b[1mhttps://huggingface.co/datasets/sergi24sanchez/codeparrot-valid\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli repo create --repo-type dataset sergi24sanchez/codeparrot-valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23903e4c-722a-48e2-9d2f-ae8ea9f7aeb0",
   "metadata": {},
   "source": [
    "# Builiding a Tokenizer\n",
    "\n",
    "Now that we have gathered and loaded our large dataset. Let's process efficiently the data to feed to our model.\n",
    "\n",
    "Previously: pretrained models --> pretrained tokenizers. Data passed through a specific preprocessing pipeline defined in the tokenizer.\n",
    "*When using a pretreined model, it's important to stick with the same preprocessing design choices selected for pretraining*. Otherwise the model may be fed out-of-distribution patterns or unknown tokens.\n",
    "\n",
    "Kinds of problems we might run into when using an existing tokenizer:\n",
    "\n",
    "- T5 tokenizer has an extensive step of stopword filtering.\n",
    "- CamemBERT only comprised French text\n",
    "\n",
    "We can test these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47ff221-8eb5-457e-aa5b-edc3087ad59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 tokend for 'sex': ['', 's', 'ex']\n",
      "CamemBERT tokend for 'being': ['be', 'ing']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tok_list(tokenizer, string):\n",
    "    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
    "    return [tokenizer.decode(tok) for tok in input_ids]\n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "print(f\"T5 tokend for 'sex': {tok_list(tokenizer_t5,'sex')}\")\n",
    "print(f\"CamemBERT tokend for 'being': {tok_list(tokenizer_camembert,'being')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9fba95-34b6-40be-ba5e-f79f472f6d03",
   "metadata": {},
   "source": [
    "## The Tokenizer Model\n",
    "\n",
    "Normalization, pretokenization, tokenizer model, and postprocessing.\n",
    "The part of the tokenizer that can be trained is the *tokenizer model* (BPE, WordPiece, and Unigram)\n",
    "\n",
    "## Measuring Tokenizer Performance\n",
    "\n",
    "- *Subword fertility*, which calculates the average number of subwords produced per tokenizer word\n",
    "- *Proportion of continued words*, refers to the proportion of tokenized words in the corpus that are split into at least two subtokens\n",
    "- *Coverage metrics* like the proportion of unknown words or rarely used tokens in a tokenized corpus\n",
    "\n",
    "Robustness to misspelling or noise is often estimated, as well as model performance on such out-of-domain examples.\n",
    "\n",
    "The measures tend to ignore the interaction of the tokenizer with the model.\n",
    "\n",
    "Generally, the performance of the tokenizer is best estimated by using the downstream performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64838b10-56fd-435b-856f-fbe6efe3467b",
   "metadata": {},
   "source": [
    "## A Tokenizer for Python\n",
    "\n",
    "If we split on whitespaces and remove them, we'll lose all the identation information.\n",
    "Line breaks are not meaningful.\n",
    "Splitting on punctuation might not make as much sense as it would in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd3625e-d04b-4b24-bb95-2d36fbb71fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '()', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "python_code = r\"\"\"def say_hello()\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Print it\n",
    "say_hello()\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b555e0-8b48-47ad-94cb-3d0c372e758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let's see what normalization is applied in the tokenizer\n",
    "print(tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e203eed-1b08-4617-adf9-88df8f3876b6",
   "metadata": {},
   "source": [
    "GPT2 tokenizer works directly on the raw Unicode.\n",
    "Pretokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901ccff4-a15b-4974-967b-28f722f2298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('()', (13, 15)), ('ĊĠĠĠ', (15, 19)), ('Ġprint', (19, 25)), ('(\"', (25, 27)), ('Hello', (27, 32)), (',', (32, 33)), ('ĠWorld', (33, 39)), ('!\")', (39, 42)), ('Ċ', (42, 43)), ('Ċ', (43, 44)), ('#', (44, 45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('say', (55, 58)), ('_', (58, 59)), ('hello', (59, 64)), ('()', (64, 66)), ('Ċ', (66, 67))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfee41-776f-4a75-9028-5d28b976d5e8",
   "metadata": {},
   "source": [
    "Ġ symbols?? And what are the numbers accompanying the tokens?\n",
    "\n",
    "Tokenizers has a very useful feature for switching between sstrings and tokens, *offset tracking*. These numbers simply indicate where in the original string each tokens comes from.\n",
    "\n",
    "Odd-looking characters. *Byte-level* means that this tokenizer works on bytes instead of Unicode characters. Each Unicode character is composed of between 1-4 bytes. While there are 143,859 Unicode characters, there are only 256 elements in the byte alphabet. If we work on bytes we can thus express al the strings composed from the UTF-8 world as longer strings in this alphabet of 256 values.\n",
    "That is, we can have a **model using an alphabet of only 256 words and be able to process any Unicode string**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d15a0ef-9a2e-498c-a6ae-d778aef60f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'a' is encoded as 'b'a'' with a single byte: 97'\n",
      "'€' is encoded as 'b'\\xe2\\x82\\xac'' with bytes: [226, 130, 172]'\n"
     ]
    }
   ],
   "source": [
    "a, e = u\"a\", u\"€\"\n",
    "byte = ord(a.encode(\"utf-8\"))\n",
    "print(f\"'{a}' is encoded as '{a.encode('utf-8')}' with a single byte: {byte}'\")\n",
    "byte = [ord(chr(i)) for i in e.encode('utf-8')]\n",
    "print(f\"'{e}' is encoded as '{e.encode('utf-8')}' with bytes: {byte}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9f9e4-d516-4dd1-9d13-2e0163f651ab",
   "metadata": {},
   "source": [
    "Why work on a byte level?\n",
    "\n",
    "Middle-ground solution: construct a medium-sized vocabulary by extending the 256-word vocabulary + most common combinations of bytes.\n",
    "\n",
    "BPE algorithm: progressively construct a vocabulary of a predefined size by creating new vocabulary tokens through iteratibely merfin the mos frequently cooccurring pair of tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670aaf32-1925-41c1-a16c-de7001ab4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of base vocab: 256\n",
      "First element: '!', last element: 'Ń''\n"
     ]
    }
   ],
   "source": [
    "# entire mapping of 256 elementary values to Unicode strings\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "bytes_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k,v in bytes_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "print(f\"Size of base vocab: {len(base_vocab)}\")\n",
    "print(f\"First element: '{base_vocab[0]}', last element: '{base_vocab[-1]}''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0318a-d527-4476-80eb-ff30818d3349",
   "metadata": {},
   "source": [
    "BPE algorithms are typically designed to work on characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3166c6f3-eb73-46c0-a0ae-f5d05bc393a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the vocab: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"size of the vocab: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f055696-16ad-415a-bd50-614424ecc4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '()', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90a9a3-5347-4350-b839-95c74f81fa4c",
   "metadata": {},
   "source": [
    "BPE tokenizer keeps most of the words but will split the mutiple spaces of our indentation into several consecutive spaces.\n",
    "This happens because this tokenizer is trained on texts where consecutive spaces are rare. The BPE model thus doesn't include a specific token in the vocabulary for indentation.\n",
    "\n",
    "**Clear case where the tokenizer is poorly suited fot the dataset's domain**\n",
    "\n",
    "-------> SOLUTION: retrain the tokenizer on the target corpus\n",
    "\n",
    "## Training a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66fceca-95f4-4f55-9518-730df9480630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', ' =================================================================', ' ----------------------------------------------------------------', '________________________________________________________________', '----------------------------------------------------------------', 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', '................................................................', '================================================================']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t,_ in tokens[:8]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67970c1-d937-443c-9c51-aae31cc756a8",
   "metadata": {},
   "source": [
    "These tokens look like separator lines that are likely to be used on forums.\n",
    "\n",
    "Now, let's look at the last words added to the vocabulary, and thus the least frequent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c663631-f9e1-4ead-a09c-2e41effa3ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ' gazed', ' informants', ' Collider', ' regress', 'ominated', ' amplification', 'Compar', '….\"', ' (/', 'Commission', ' Hitman']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t,_ in tokens[:12]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e60d9-c202-4122-8c49-d80d0552860f",
   "metadata": {},
   "source": [
    "For each of these tokens our model will have to learn an associated word embedding, and we don't want the embedding matrix to contain too many noisy words.\n",
    "\n",
    "Note the very time- and space-specific words, already embedded at a low level in our modeling approach. Such specific tokens by a BPE tokenizer can also be an indication that the target vocabulary size is too larfe or that the corpus contains idiosyncratic tokens.\n",
    "\n",
    "Let's train a fresh tokenizer and anayse its learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd5143f-a7c9-4f78-ac64-e810fafe630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4122eacdecf84321ad303ae23effc01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdfdb7d5f014653b87d472d290d6ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's select 1-2GB of data (about 100K documents)\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "length = 100000\n",
    "dataset_name = \"sergi24sanchez/codeparrot-train\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "def batch_iterator(batch_size=10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=12500, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4ecff95-79ef-4b93-86a9-7c2d6e6e8f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "['  ', '    ', '   ', '\\n    ', 'se', 'in', 're', 'on', 'te', 'or', '\\n       ', 'st', 'de', '\\n   ', '        ', 'th', ' =', 'le', ' a', 'lf', 'ti', 'me', 'self']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print(len(new_tokenizer.vocab))\n",
    "print([f'{new_tokenizer.convert_tokens_to_string([t])}' for t,_ in tokens[257:280]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d940fa94-56d8-453a-b262-8e6657dd1b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tack', ' plus', 'í', 'TestResult', ' calc', 'authorize', 'El', 'Patch', 'passed', 'srs', 'xi', ' Twisted']\n"
     ]
    }
   ],
   "source": [
    "print([f\"{new_tokenizer.convert_tokens_to_string([t])}\" for t,_ in tokens[-12:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1465b128-3b0e-4e5f-a5e8-5ef801eeebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġs', 'ay', '_', 'hello', '()', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\")', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 's', 'ay', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "# tokenize our simple example\n",
    "print(new_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4e8e0e6-c112-4a30-ac13-2ce774044f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 35 Python keywords.\n",
      "No, keyword 'await' is not in the vocabulary\n",
      "No, keyword 'finally' is not in the vocabulary\n",
      "No, keyword 'nonlocal' is not in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# check if all the python reserved keywords are in the vocab\n",
    "import keyword\n",
    "\n",
    "print(f\"There are in total {len(keyword.kwlist)} Python keywords.\")\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer.vocab:\n",
    "        print(f\"No, keyword '{keyw}' is not in the vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fb9af-b07e-4959-a974-9bf0aad155cf",
   "metadata": {},
   "source": [
    "Let's try building a larger vocab using a larger sample of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5a884b1-d7a8-4873-911a-6026f6c4a961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6b3df987ab4e4e9a6c13b610051702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = 200000\n",
    "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=32768, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6861ca0d-28ad-48be-bea6-3402368656de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' hexlify', 'partitioner', ' Elements', 'innerHTML', '仓', 'SomeClass', 'tlvs', ' siblings', 'hicles', 'Survey', 'ManagementClient', ' 為']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print([f'{new_tokenizer_larger.convert_tokens_to_string([t])}' for t,_ in tokens[-12:]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc96c87e-4210-4fa9-ab79-370f37c52195",
   "metadata": {},
   "source": [
    "It doesn't show any regular programming keywords here, which is promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "525c64cb-3624-4c3f-8d56-4e8346cf9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '()', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\")', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer_larger(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b51caa-33fe-4087-a389-ab5fb40e225c",
   "metadata": {},
   "source": [
    "Indents are conveniently kept. This seems more in line with our expectations of the data the model may see in the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12293084-e4e7-4152-8139-893647ae1223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, keyword 'nonlocal' is not in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer_larger.vocab:\n",
    "        print(f\"No, keyword '{keyw}' is not in the vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188bdc0-ee2d-413f-af78-0df820880240",
   "metadata": {},
   "source": [
    "We'll measure the performance of the tokenizer basd on the model's performance.\n",
    "\n",
    "## Saving a Custom Tokenizer on the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2193b2bc-6294-46bf-ac5a-e3fdffa17bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/transformers/utils/hub.py:731: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sergi24sanchez/codeparrot/commit/889d8522a8e499a08d4dd4a4999ce301a33b830c', commit_message='Upload tokenizer', commit_description='', oid='889d8522a8e499a08d4dd4a4999ce301a33b830c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/sergi24sanchez/codeparrot', endpoint='https://huggingface.co', repo_type='model', repo_id='sergi24sanchez/codeparrot'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = \"codeparrot\"\n",
    "org = \"sergi24sanchez\"\n",
    "new_tokenizer_larger.push_to_hub(model_ckpt, organization=org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52a475e0-3345-4d71-b846-831661f53c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78168b535bd64f9283ab0d9fa7f8d83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916ff9af33d5459cb84834d1acf82c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a19634fa19242d3b89b39430921d676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdf9ebf3bd043808daeb65f25486fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbcd27eb2fe4977be3854bcdd64fddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '()', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\")', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(org + '/' + model_ckpt)\n",
    "print(reloaded_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cda23844-7631-4f52-8bb6-fe8e716afbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sergi24sanchez/codeparrot-small_vocabulary/commit/8f85c7fe02327913415846bed11a88381c4e852f', commit_message='Upload tokenizer', commit_description='', oid='8f85c7fe02327913415846bed11a88381c4e852f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/sergi24sanchez/codeparrot-small_vocabulary', endpoint='https://huggingface.co', repo_type='model', repo_id='sergi24sanchez/codeparrot-small_vocabulary'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.push_to_hub(org + '/' + model_ckpt + \"-small_vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d4d07-d606-4f26-8e94-cdf3b2b890e2",
   "metadata": {},
   "source": [
    "# Training a Model from Scratch\n",
    "\n",
    "Which architecture works best for the task, initialize a fresh model without pretrained weights, set up a custom data laoding class, and create a scalable training loop.\n",
    "\n",
    "*In this section a longer thatn usual script to train a model on a distributed infrastructure will be implemented. You should not run each code snippet independently, but instead download the script provided in the [Transformers repository](https://github.com/huggingface/transformers-research-projects/tree/main/codeparrot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75635e7a-1536-42d4-bc97-04cdd355dbbb",
   "metadata": {},
   "source": [
    "## Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9356a1b-3961-47d6-93ef-951202b2b4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"sergi24sanchez/codeparrot\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e116ff-d0ed-4946-910e-df809cc4abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 (xl) size: 1529.6M parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT-2 (xl) size: {model_size(model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226eb4e6-e30b-4f86-8cf2-28fe70dcf4c5",
   "metadata": {},
   "source": [
    "In general, large language models are more efficient to train as long as the dataset is reasonably large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93af691c-84a1-40e0-98f3-b3e070e50dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f329e6a3295483fa1db56cdbbcfc353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d3229a2a2d498caca108e9428d9979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e528eb14eee547a1bb986f6efe05f2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc6b0fa1aa44bfda60710dc40b8b8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774a13e-3d72-4824-a91c-bed3eef9f75e",
   "metadata": {},
   "source": [
    "We'll also create a smaller version that we can train to make sure everything works before scaling up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8759c8a8-c902-464f-8444-e9803d06a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "config_small = AutoConfig.from_pretrained(\"gpt2\", vocab_size=len(tokenizer))\n",
    "model_small = AutoModelForCausalLM.from_config(config_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f38a733-8893-4c27-a652-3fb0e06c8c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 111.0M parameters\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT-2 size: {model_size(model_small)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e0db0cc-600f-4f88-939a-ae131869568a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9126e47a6e3f4fe9a53a9c1951c64cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/444M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_small.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3b7a3-930e-4340-9687-92a35a0f3045",
   "metadata": {},
   "source": [
    "## Implementing the Dataloader\n",
    "\n",
    "To be able to train with maximal efficiency, we will want to supply our model with sequences filling its context. For instance, if the context length of our model is 1024 tokens we always want to provide 1024-token sequences during training.\n",
    "\n",
    "To feed batches with full sequences of *sequence_length* to our model, we should thus either drop the last incomplete sequence or pad it. However, this will render our training slightly less efficient and force us to take care of padding and masking padded token labels.\n",
    "\n",
    "We are much more compute- than data-constrained, se we'll take the easy and efficient way here: we can tokenize several examples and then concatenate them, separated by the special end-of-sequence token, to get a very long sequence. Finally, we split this sequence into equally sized chunks. With this approach, we lose at most a small fraction of the data at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea39b283-9649-413a-9a0c-13cc22ace16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our input string character length as:\n",
    "# input_characters = number_of_sequences * sequence_length * characters_per_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0800784-b074-40c1-8a49-c8980fdd58d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fc533355844708a4d4e3a0529865de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae3731bd8674fa3942a371642004b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1842 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.814367450731087\n"
     ]
    }
   ],
   "source": [
    "# estimate the average character length per token in our dataset:\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "examples, total_characters, total_tokens = 500, 0, 0\n",
    "dataset = load_dataset(\"sergi24sanchez/codeparrot-train\", split=\"train\", streaming=True)\n",
    "\n",
    "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
    "    total_characters += len(example['content'])\n",
    "    total_tokens += len(tokenizer(example['content']).tokens())\n",
    "\n",
    "characters_per_token = total_characters / total_tokens\n",
    "print(characters_per_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d00fa-cc97-440b-bbaa-02e90b176402",
   "metadata": {},
   "source": [
    "With that we hace all that's needed to create our own *IterableDataset* (which is a helper class provided by PyTorch) for preparing constant-length inputs for the model. We need to inherit and set up the __iter__() function that yields the next element with the logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88cb83a8-1d83-4574-912d-48d75ed104c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, tokenizer, dataset, seq_length=1024,\n",
    "                 num_of_sequences=1024, chars_per_token=3.8):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "        self.current_size = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    m = f\"Buffer full: {buffer_len}>= {self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    break\n",
    "                try:\n",
    "                    m = f\"Fill buffer: {buffer_len} < {self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    buffer.append(next(iterator)[\"content\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    iterator = iter(self.dataset)\n",
    "\n",
    "            all_token_ids = []\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n",
    "            for tokenized_input in tokenized_inputs[\"input_ids\"]:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    self.current_size += 1\n",
    "                    yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f084f8b-bd69-4cb0-985a-4256b09cf383",
   "metadata": {},
   "source": [
    "The *__iter__()* function builds up a buffer of strings until it contains enough characters. All the elements in the buffer are tokenized and concatenated with the EOS token, then the long sequence in *all_token_ids* is chuncked in *seq_length*-sized slices.\n",
    "\n",
    "Normally, we need attention masks to stack padded sequences of varying length and make sure the padding is ignored during training. We have taken care of this by only providing sequences of the same (maximal) length.\n",
    "\n",
    "Let's test our iterable dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea22ba44-986a-41e6-8f7f-72cecb696a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill buffer: 0 < 38912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='cdn-lfs-us-1.hf.co', port=443): Max retries exceeded with url: /repos/bb/5c/bb5cce8bcb015398753c0cee2dbaf1b7300b5bac7a676a2963423059dca3c97d/e1318d83ba4e39b1536336a2dc14c02ac19e4d091f644000edb60a2cb7f8964d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27resultados-000000000391.json.gz%3B+filename%3D%22resultados-000000000391.json.gz%22%3B&response-content-type=application%2Fgzip&Expires=1751804093&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTgwNDA5M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2JiLzVjL2JiNWNjZThiY2IwMTUzOTg3NTNjMGNlZTJkYmFmMWI3MzAwYjViYWM3YTY3NmEyOTYzNDIzMDU5ZGNhM2M5N2QvZTEzMThkODNiYTRlMzliMTUzNjMzNmEyZGMxNGMwMmFjMTllNGQwOTFmNjQ0MDAwZWRiNjBhMmNiN2Y4OTY0ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=JozBGbllgqwgxbhQPZUrYFMAk1r4C7wVGQqmJapXvIR-oyV4WgM6vBpMGDARrvt5otjx4ILr5jFackEVGwXCeqf31w6mBLlblWk9ZFVJA9aOojOyG3bOaBU8dBml-py7kELQ7jMrv-FptAoqW772UuWXj4LRl8wvvK15xdk5syeq6CrerSv659qjzHBMHzN7I-M2WfUZi9~VCjY-WmyOVnd4B4g1eVNCEhHJLky0ve3Sf97WQWE0H1DwpeSV7kf8JialNITNxbeb9vHNi6RacPYvugnHpT9a1~I8RP499u2mdS-mFKkadnIuXr44GNNydEiM8LO1Cay00pV1CuNnzg__&Key-Pair-Id=K24J24Z295AEI9 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f6177724850>: Failed to establish a new connection: [Errno -2] Name or service not known'))\"), '(Request ID: 0c2ef402-c0e8-4569-8396-ee554df76448)')' thrown while requesting GET https://huggingface.co/datasets/sergi24sanchez/codeparrot-train/resolve/e2e3767f3b1556eb367c365efc64351a1fe668fd/resultados-000000000391.json.gz\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fill buffer: 3148 < 38912\n",
      "Fill buffer: 4549 < 38912\n",
      "Fill buffer: 14549 < 38912\n",
      "Fill buffer: 22128 < 38912\n",
      "Fill buffer: 25447 < 38912\n",
      "Fill buffer: 27179 < 38912\n",
      "Fill buffer: 31295 < 38912\n",
      "Fill buffer: 35696 < 38912\n",
      "Fill buffer: 36747 < 38912\n",
      "Buffer full: 39014>= 38912\n",
      "Lengths of the sequences: [1024, 1024, 1024, 1024, 1024]\n"
     ]
    }
   ],
   "source": [
    "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
    "constant_length_dataset = ConstantLengthDataset(tokenizer, shuffled_dataset, num_of_sequences=10)\n",
    "dataset_iterator = iter(constant_length_dataset)\n",
    "\n",
    "lengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\n",
    "print(f\"Lengths of the sequences: {lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae6ab1-f8b8-4536-9210-e978ead464fa",
   "metadata": {},
   "source": [
    "Nice! This works as intended and we get constant-length inputs for the model.\n",
    "\n",
    "Now that we hace a reliable data source for the model, it's time to build the actual training loop.\n",
    "\n",
    "## Defining the Training Loop\n",
    "\n",
    "Even on modern graphics card you can't train a model at GPT-2 scale in *reasonable time*.\n",
    "\n",
    "We will implement **data parallelism**, which will help us utilize several GPUs for training.\n",
    "We can use Accelerate to make our code scalable, which is designed to make distributed training--and changing the underlying hardware for training-- easy. We can also use the *Trainer* for distibuted training but Accelerate gives us full over the training loop.\n",
    "\n",
    "Accelerate provides an easy API to make training scripts run with mized precision and in nay kinf of distributed setting.\n",
    "The same code can then run seamlessly on your local machine for debugging purposes or your beefy training cluster for the final training run.\n",
    "Few changes to your native PyTorch training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6d80c-f47e-4a6b-9a24-64bd2d882a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model = torch.nn.Transformer()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "dataset = load_dataset(\"sergi24sanchez/codeparrot-train\", split=\"train\", streaming=True)\n",
    "data = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    for source, targets in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(source)\n",
    "        loss = F.cross_entropy(output, targets)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3aaa4-1fbf-4719-9708-b9c78bcf0fbd",
   "metadata": {},
   "source": [
    "The core part of the changes is the call to *prepare()*, which makes sure the model, optimizers, and dataloaders are all prepared and distributed on the infrastructure. These minor changes to the Pytorch training loop enable you to easily scale training scross different infrastructures.\n",
    "\n",
    "With that in mind, let's start building up our training script and define a few helper functions.\n",
    "1st, we set up the hyperparameters for training and wrap them in a *Namespace* for easy access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3669d1ce-7027-4ffb-ad9c-098794b637a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# Commented parameters correspond to the small model\n",
    "config = {\n",
    "    \"train_batch_size\": 2, # 12\n",
    "    \"valid_batch_size\": 2, # 12\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"shuffle_buffer\": 1000,\n",
    "    \"learning_rate\": 2e-4, # 5e-4\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_warmup_steps\": 750, # 2000\n",
    "    \"gradient_accumulation_steps\": 16, # 1\n",
    "    \"max_train_steps\": 50000, # 150000\n",
    "    \"max_eval_steps\": -1,\n",
    "    \"seq_length\": 1024,\n",
    "    \"seed\": 1,\n",
    "    \"save_checkpoint_steps\": 50000, #15000\n",
    "}\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818c9de-f413-42ba-812d-8457d81947eb",
   "metadata": {},
   "source": [
    "Next, we set up logging for training. Since we are training a model from scratch, the treaining run will take a while and require expensive infrastructure. Therefore, we want to make sure that all the relevant information is stored and easily accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f0ed4-b597-44d2-bef8-dd6c6e662ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "def setup_logging(project_name):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO,\n",
    "        handlers=[\n",
    "            logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    if accelerator.is_main_process: # we only want to set up logging once\n",
    "        wandb.init(project=project_name, config=args)\n",
    "        run_name = wandb.run.name\n",
    "        tb_writer = SummaryWriter()\n",
    "        tb_writer.add_hparams(vars(args), {'0': 0})\n",
    "        logger.setLevel(logging.INFO)\n",
    "        datasets.utils.logging.set_verbosity_debug()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        tb_writer = None\n",
    "        run_name = ''\n",
    "        logger.setLevel(logging.ERROR)\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    return logger, tb_writer, run_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00dba2-cead-4dab-a056-dc8fe925cb2c",
   "metadata": {},
   "source": [
    "Each worker gets a unique accelerator.process_index, which we use with the FileHandler to write the logs of each worker to an individual file.\n",
    "\n",
    "We also use the accelerator.is_main_process attribute, which is only true for the main worker. We make sure we don't initialize the Tensorboard and Weights&Biases loggers several times.\n",
    "\n",
    "Ww'll also define a function to log the metrics with TensorBoard and W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e226bf-b3f5-409e-8489-ea0ba357a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(step, metrics):\n",
    "    logger.info(f\"Step {step}: {metrics}\")\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.log(metrics)\n",
    "        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e7553-0c0c-4061-b293-a60ef5f73e4b",
   "metadata": {},
   "source": [
    "Let's write a function that creates the dataloaders for the training and validation sets with our brand new *ConstantLengthDataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fbdfcf-4bf8-4c55-ab31-0ec48db5a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "def create_dataloaders(dataset_name):\n",
    "    train_data = load_dataset(dataset_name+'-train', split=\"train\", streaming=True)\n",
    "    train_data = train_data.shuffle(buffer_size=args.suffle_buffer, seed=arags.seed)\n",
    "    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\", streaming=True)\n",
    "    \n",
    "    train_dataset = ConstantLengthDataset(tokenizer, train_data, seq_length=args.seq_length)\n",
    "    valid_dataset = ConstantLengthDataset(tokenizer, valid_data, seq_length=args.seq_length)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
    "    eval_dataloader = DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n",
    "    return train_dataloader, eval_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1ff6c-0cc9-42fe-9d50-021bdaaf6b73",
   "metadata": {},
   "source": [
    "Accelerate will take care of distributing the batches to each worker.\n",
    "\n",
    "Another aspect to implement is optimization. We will set up the oprimizer and learning rate schedule in the main loop, but we define a helper function here to differentiate the parameters that should receive weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3979128-00f2-42ff-a140-c97db9ad1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [{'params': params_with_wd, 'weight_decay': args.weight_decay},\n",
    "           {'params': params_without_wd, 'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d71b5-6b13-4133-aaa5-9ec347e365c0",
   "metadata": {},
   "source": [
    "Finally, we want to evaluate the model on the validation set from time to time.\n",
    "Calculate the perplexity on the evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96cae5b-f711-4d44-b2fa-ca9eb41eb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss.repeat(args.valid_batch_size)\n",
    "        losses.append(accelerator.gather(loss))\n",
    "        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598f317-61c3-48bf-a192-8baf1d6510ab",
   "metadata": {},
   "source": [
    "The perplexity measures how well the model's output probability distibutions pedict the targeted tokens. The lower the better.\n",
    "\n",
    "Before we put it all together in the training script...\n",
    "\n",
    "With the *Repository* class from the huggingface_hub you ca programmatically access the repository and pull, branch, commit or push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fedbab-ec71-44a3-95de-81b8e3964797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
