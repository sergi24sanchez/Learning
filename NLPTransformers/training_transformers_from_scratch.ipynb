{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f47a4e-2a96-49e3-9ffd-8c6039b38014",
   "metadata": {},
   "source": [
    "Comparison text generations from GPT and GPT-2 to illustrate the notion of a model being skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1d6f10-78a5-4c7b-a497-74955e692ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb86ab156e54f70a39758ff7ff46128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97242be676894e0997d4827abca76652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2025-07-01T14:46:16.894739Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, url: \\\"https://cas-server.xethub.hf.co/reconstruction/986657f8b77a4a690d73500aa22cbda25b96a55bd9b689e8312ebd983a7108d5\\\", source: hyper_util::client::legacy::Error(Connect, ConnectError(\\\"dns error\\\", Custom { kind: Uncategorized, error: \\\"failed to lookup address information: Name or service not known\\\" })) }). Retrying...\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":200}\n",
      "{\"timestamp\":\"2025-07-01T14:46:16.894830Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 651.203428ms before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533b818b7e64432fa99426bf02161474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b30f58e0ffd4bee887a34445692bc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa0e60b62aa4cceaea897505b7685b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be28df425204cf6b08c177c25bb9062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9c958da2264b1bac1f6430eed703c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
    "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9918d9-ebe3-4350-8779-5e98a0472a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT size:116.5M parameters\n",
      "GPT2 size:124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f\"GPT size:{model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
    "print(f\"GPT2 size:{model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a33f24-0706-4e38-b4d2-a6ff734693de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT completions:\n",
      "1.\n",
      "When they came back from the cemetery and said their good - byes at the gravesite. \n",
      " at first i was so happy to have them return to me, i couldn't contain myself. with the relief we could share and the sense of relief they\n",
      "2.\n",
      "When they came back into the room. they were smiling at me and i thought i saw a hint of relief. we were all exhausted as we headed down the hallway and into the living room. \n",
      " \" where have you been? we've been so\n",
      "3.\n",
      "When they came back out to the clearing she was with, but they didn't see her. when she stopped by the river to drink from the spring, she saw a strange object floating across toward her. then, as suddenly as it had appeared,\n",
      "\n",
      "GPT2 completions:\n",
      "1.\n",
      "When they came back, there was a small group of us waiting for them there as our security guard stood by them, then they began to run, kicking, slashing and trying to frighten me into silence. It took about ten minutes before they\n",
      "2.\n",
      "When they came back, the car was already moving.\n",
      "\n",
      "\"There was a guy who was wearing a hooded sweatshirt that was holding a stick at the foot of his chest,\" says a relative who knew the man, calling himself Robert\n",
      "3.\n",
      "When they came back for it, they decided not to try it out because you need to be careful with the weight (and you will burn lots of fuel).\n",
      "\n",
      "\n",
      "We are now back in business!! We think of the next thing we need\n"
     ]
    }
   ],
   "source": [
    "def enum_pipeline_outputs(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(prompt, num_return_sequences=num_return_sequences, clean_up_tokenization_spaces=True)\n",
    "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
    "\n",
    "prompt = \"\\nWhen they came back\"\n",
    "print(\"GPT completions:\\n\" + enum_pipeline_outputs(generation_gpt, prompt, 3))\n",
    "print(\"\")\n",
    "print(\"GPT2 completions:\\n\" + enum_pipeline_outputs(generation_gpt2, prompt, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d4449-a5a1-4640-b70e-5910a195b10c",
   "metadata": {},
   "source": [
    "## Building a Custom Code Dataset\n",
    "### Creating a dataset with Google BigQuery\n",
    "\n",
    "tota la perafernalia que he hagut de fer per executar una consulta, guardar els resultats en una taula d'un dataset i passar les dades a una carpeta del bucket 'npl_transformers'.\n",
    "Després he instalat el [gcloud CLI](https://cloud.google.com/sdk/docs/install) al servidor.\n",
    "Llavors he pogut llançar la comana: ``gsutil -m -o \"GSUtil:parellel_process_count=1\" cp -r gs:<nom_del_bucket>/<carpeta>/resultados-*.json.gz /datasets/codeparrot``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7403ba6-471b-4174-9674-929dcee53163",
   "metadata": {},
   "source": [
    "## Working with Large Datasets\n",
    "\n",
    "Loading a very large dataset is often a challenging task, in particular when the data is larger than your machine's RAM.\n",
    "In our example, we have 50 GB of compressed data and about 200 GB of uncompressed data.\n",
    "\n",
    "Thankfully Datasets has two specific features that allow you to set yourself free from RAM and hard drive space limitations: **memory mapping** and **streaming**.\n",
    "\n",
    "### Memory mapping\n",
    "\n",
    "Overcoming RAM limitations --> uses a mechanism of zero-copy and zero-overhead memory mapping.\n",
    "Basically, each dataset is cached on the drive in a file that is a direct reflection of the content in RAM memory. Instead of loading the dataset in RAM, it opens a read-only pointer to this file and uses it as a substitue for RAM (using hard drive as an extension of RAM).\n",
    "\n",
    "Here, we will direcly load our 50GB of compressed JSON files that we have stored locally in the *codeparrot* repository.\n",
    "Decompress JSON files. Be careful, cause this uses **180GB of free disk!!!**\n",
    "\n",
    "*delete_extracted=True* in the dataset's downloading configuration, we can make sure that we delete all the files we don't need anymore as soon as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3cf18bd-102f-4341-af46-cc2190a65d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf4fe40d09c4d3d9e7f24d261c7e8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a2c79b4abb469db2f0b77536aa196a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/500 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2370ad6485a54ccfb34c8a6ab90c60fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062f580d205b4fe283a501090776c5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DownloadConfig, Features, Value\n",
    "import os\n",
    "\n",
    "download_config = DownloadConfig(delete_extracted=True)\n",
    "dataset_path = os.path.expanduser(\"~/datasets/codeparrot/export_results/*.json.gz\")\n",
    "\n",
    "features = Features({\n",
    "    \"repo_name\": Value(\"string\"),\n",
    "    \"path\": Value(\"string\"),\n",
    "    \"copies\": Value(\"string\"),\n",
    "    \"size\": Value(\"string\"),\n",
    "    \"content\": Value(\"string\"),\n",
    "    \"license\": Value(\"string\")\n",
    "})\n",
    "\n",
    "dataset = load_dataset(\"json\",\n",
    "                       data_files=dataset_path, split=\"train\",\n",
    "                      download_config=download_config, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4e59bb-79f9-40e0-aa84-8b5bfc6f837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gunzip -c ~/datasets/codeparrot/export_results/resultados-000000000000.json.gz > ./uno.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd876c86-9b6f-4b32-adca-ad959d6110c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d1f510e39e429bac1007878423b674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
      "    num_rows: 38184\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#from datasets import load_dataset\n",
    "#\n",
    "#dataset = load_dataset(\"json\", data_files=\"./uno.json\", split=\"train\")\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94616d64-217a-44d7-8bf9-2861e7a77d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of python files code in dataset : 18923569\n",
      "Dataset size (cache file) : 97.06 GB\n",
      "RAM used: 1720 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"Number of python files code in dataset : {len(dataset)}\")\n",
    "ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
    "# os.stat.st_size is expressed in bytes, so we convert to GB\n",
    "print(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\n",
    "# Process.memory_ingo is expressed in bytes, so we convert to MB\n",
    "print(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b19efb-3b4b-4223-9baa-5d80d233e32f",
   "metadata": {},
   "source": [
    "What if you can't free enough disk space to store the full dataset locally?\n",
    "\n",
    "## Streaming\n",
    "\n",
    "An alternative to scaling up the server you are using is to *stream* the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1894aab4-4113-44ec-89b4-24dcd77274ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa73b0bbc5a440a497246a3f5d008b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "streamed_dataset = load_dataset(\"json\",\n",
    "                       data_files=dataset_path, split=\"train\",\n",
    "                      download_config=download_config, features=features,\n",
    "                               streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9d223-0de4-4bd2-bbf2-a7ee7e495188",
   "metadata": {},
   "source": [
    "In streaming mode, the compressed JSON files will be opened and read on the fly. Out dataset is now an *IterableDataset* object. This means that we cannot access random elements of it (streamed_dataset[1264]), but we need to read it in order, for instance with ``next(iter(streamed_dataset))``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c44e734d-04e7-4d90-b579-cc323fe65daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(streamed_dataset)\n",
    "\n",
    "print(dataset[0] == next(iterator))\n",
    "print(dataset[1] == next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed7e7a-81c1-4408-91e1-eba7e74fb36e",
   "metadata": {},
   "source": [
    "The original raw files are extracted and read on the fly when a new batch of examples is requested, and onñy that batch is loaded in memory.\n",
    "\n",
    "One step further-- instead of pointing to the local dataset we can reference the dataset on the Hub, and then directly download samples without downloading the raw files locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0145c66d-df0c-40bb-8adc-197e1a71126a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9a3927804842c2bcd0a342d438bc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba046b8708a47cea9c872f3e28f2e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remote_dataset = load_dataset('transformersbook/codeparrot', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8f38625-c75e-4387-88c7-d0449548fc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb1a0ed048f43a08ac959a902e45ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "658476d6-9ac8-4978-b7ae-98f3503bcb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe --type argument is deprecated and will be removed in a future version. Use --repo-type instead.\u001b[0m\n",
      "\u001b[33mThe --organization argument is deprecated and will be removed in a future version. Pass the organization namespace directly in the repo_id.\u001b[0m\n",
      "Successfully created \u001b[1msergi24sanchez/codeparrot-train\u001b[0m on the Hub.\n",
      "Your repo is now available at \u001b[1mhttps://huggingface.co/datasets/sergi24sanchez/codeparrot-train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli repo create --repo-type dataset --organization sergi24sanchez codeparrot-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcf855d0-e4c0-403b-85a9-d6f2cf17d11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe --organization argument is deprecated and will be removed in a future version. Pass the organization namespace directly in the repo_id.\u001b[0m\n",
      "Successfully created \u001b[1msergi24sanchez/codeparrot-valid\u001b[0m on the Hub.\n",
      "Your repo is now available at \u001b[1mhttps://huggingface.co/datasets/sergi24sanchez/codeparrot-valid\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli repo create --repo-type dataset sergi24sanchez/codeparrot-valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23903e4c-722a-48e2-9d2f-ae8ea9f7aeb0",
   "metadata": {},
   "source": [
    "# Builiding a Tokenizer\n",
    "\n",
    "Now that we have gathered and loaded our large dataset. Let's process efficiently the data to feed to our model.\n",
    "\n",
    "Previously: pretrained models --> pretrained tokenizers. Data passed through a specific preprocessing pipeline defined in the tokenizer.\n",
    "*When using a pretreined model, it's important to stick with the same preprocessing design choices selected for pretraining*. Otherwise the model may be fed out-of-distribution patterns or unknown tokens.\n",
    "\n",
    "Kinds of problems we might run into when using an existing tokenizer:\n",
    "\n",
    "- T5 tokenizer has an extensive step of stopword filtering.\n",
    "- CamemBERT only comprised French text\n",
    "\n",
    "We can test these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47ff221-8eb5-457e-aa5b-edc3087ad59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 tokend for 'sex': ['', 's', 'ex']\n",
      "CamemBERT tokend for 'being': ['be', 'ing']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tok_list(tokenizer, string):\n",
    "    input_ids = tokenizer(string, add_special_tokens=False)[\"input_ids\"]\n",
    "    return [tokenizer.decode(tok) for tok in input_ids]\n",
    "\n",
    "tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "tokenizer_camembert = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "print(f\"T5 tokend for 'sex': {tok_list(tokenizer_t5,'sex')}\")\n",
    "print(f\"CamemBERT tokend for 'being': {tok_list(tokenizer_camembert,'being')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9fba95-34b6-40be-ba5e-f79f472f6d03",
   "metadata": {},
   "source": [
    "## The Tokenizer Model\n",
    "\n",
    "Normalization, pretokenization, tokenizer model, and postprocessing.\n",
    "The part of the tokenizer that can be trained is the *tokenizer model* (BPE, WordPiece, and Unigram)\n",
    "\n",
    "## Measuring Tokenizer Performance\n",
    "\n",
    "- *Subword fertility*, which calculates the average number of subwords produced per tokenizer word\n",
    "- *Proportion of continued words*, refers to the proportion of tokenized words in the corpus that are split into at least two subtokens\n",
    "- *Coverage metrics* like the proportion of unknown words or rarely used tokens in a tokenized corpus\n",
    "\n",
    "Robustness to misspelling or noise is often estimated, as well as model performance on such out-of-domain examples.\n",
    "\n",
    "The measures tend to ignore the interaction of the tokenizer with the model.\n",
    "\n",
    "Generally, the performance of the tokenizer is best estimated by using the downstream performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64838b10-56fd-435b-856f-fbe6efe3467b",
   "metadata": {},
   "source": [
    "## A Tokenizer for Python\n",
    "\n",
    "If we split on whitespaces and remove them, we'll lose all the identation information.\n",
    "Line breaks are not meaningful.\n",
    "Splitting on punctuation might not make as much sense as it would in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd3625e-d04b-4b24-bb95-2d36fbb71fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '()', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "python_code = r\"\"\"def say_hello()\n",
    "    print(\"Hello, World!\")\n",
    "\n",
    "# Print it\n",
    "say_hello()\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b555e0-8b48-47ad-94cb-3d0c372e758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let's see what normalization is applied in the tokenizer\n",
    "print(tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e203eed-1b08-4617-adf9-88df8f3876b6",
   "metadata": {},
   "source": [
    "GPT2 tokenizer works directly on the raw Unicode.\n",
    "Pretokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "901ccff4-a15b-4974-967b-28f722f2298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('def', (0, 3)), ('Ġsay', (3, 7)), ('_', (7, 8)), ('hello', (8, 13)), ('()', (13, 15)), ('ĊĠĠĠ', (15, 19)), ('Ġprint', (19, 25)), ('(\"', (25, 27)), ('Hello', (27, 32)), (',', (32, 33)), ('ĠWorld', (33, 39)), ('!\")', (39, 42)), ('Ċ', (42, 43)), ('Ċ', (43, 44)), ('#', (44, 45)), ('ĠPrint', (45, 51)), ('Ġit', (51, 54)), ('Ċ', (54, 55)), ('say', (55, 58)), ('_', (58, 59)), ('hello', (59, 64)), ('()', (64, 66)), ('Ċ', (66, 67))]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfee41-776f-4a75-9028-5d28b976d5e8",
   "metadata": {},
   "source": [
    "Ġ symbols?? And what are the numbers accompanying the tokens?\n",
    "\n",
    "Tokenizers has a very useful feature for switching between sstrings and tokens, *offset tracking*. These numbers simply indicate where in the original string each tokens comes from.\n",
    "\n",
    "Odd-looking characters. *Byte-level* means that this tokenizer works on bytes instead of Unicode characters. Each Unicode character is composed of between 1-4 bytes. While there are 143,859 Unicode characters, there are only 256 elements in the byte alphabet. If we work on bytes we can thus express al the strings composed from the UTF-8 world as longer strings in this alphabet of 256 values.\n",
    "That is, we can have a **model using an alphabet of only 256 words and be able to process any Unicode string**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d15a0ef-9a2e-498c-a6ae-d778aef60f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'a' is encoded as 'b'a'' with a single byte: 97'\n",
      "'€' is encoded as 'b'\\xe2\\x82\\xac'' with bytes: [226, 130, 172]'\n"
     ]
    }
   ],
   "source": [
    "a, e = u\"a\", u\"€\"\n",
    "byte = ord(a.encode(\"utf-8\"))\n",
    "print(f\"'{a}' is encoded as '{a.encode('utf-8')}' with a single byte: {byte}'\")\n",
    "byte = [ord(chr(i)) for i in e.encode('utf-8')]\n",
    "print(f\"'{e}' is encoded as '{e.encode('utf-8')}' with bytes: {byte}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9f9e4-d516-4dd1-9d13-2e0163f651ab",
   "metadata": {},
   "source": [
    "Why work on a byte level?\n",
    "\n",
    "Middle-ground solution: construct a medium-sized vocabulary by extending the 256-word vocabulary + most common combinations of bytes.\n",
    "\n",
    "BPE algorithm: progressively construct a vocabulary of a predefined size by creating new vocabulary tokens through iteratibely merfin the mos frequently cooccurring pair of tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670aaf32-1925-41c1-a16c-de7001ab4773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of base vocab: 256\n",
      "First element: '!', last element: 'Ń''\n"
     ]
    }
   ],
   "source": [
    "# entire mapping of 256 elementary values to Unicode strings\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "bytes_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v, k) for k,v in bytes_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "print(f\"Size of base vocab: {len(base_vocab)}\")\n",
    "print(f\"First element: '{base_vocab[0]}', last element: '{base_vocab[-1]}''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d0318a-d527-4476-80eb-ff30818d3349",
   "metadata": {},
   "source": [
    "BPE algorithms are typically designed to work on characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3166c6f3-eb73-46c0-a0ae-f5d05bc393a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the vocab: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f\"size of the vocab: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f055696-16ad-415a-bd50-614424ecc4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '()', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\"', ')', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90a9a3-5347-4350-b839-95c74f81fa4c",
   "metadata": {},
   "source": [
    "BPE tokenizer keeps most of the words but will split the mutiple spaces of our indentation into several consecutive spaces.\n",
    "This happens because this tokenizer is trained on texts where consecutive spaces are rare. The BPE model thus doesn't include a specific token in the vocabulary for indentation.\n",
    "\n",
    "**Clear case where the tokenizer is poorly suited fot the dataset's domain**\n",
    "\n",
    "-------> SOLUTION: retrain the tokenizer on the target corpus\n",
    "\n",
    "## Training a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c66fceca-95f4-4f55-9518-730df9480630",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', ' =================================================================', ' ----------------------------------------------------------------', '________________________________________________________________', '----------------------------------------------------------------', 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ', '................................................................', '================================================================']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t,_ in tokens[:8]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67970c1-d937-443c-9c51-aae31cc756a8",
   "metadata": {},
   "source": [
    "These tokens look like separator lines that are likely to be used on forums.\n",
    "\n",
    "Now, let's look at the last words added to the vocabulary, and thus the least frequent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c663631-f9e1-4ead-a09c-2e41effa3ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ' gazed', ' informants', ' Collider', ' regress', 'ominated', ' amplification', 'Compar', '….\"', ' (/', 'Commission', ' Hitman']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(tokenizer.vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "print([f'{tokenizer.convert_tokens_to_string([t])}' for t,_ in tokens[:12]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e60d9-c202-4122-8c49-d80d0552860f",
   "metadata": {},
   "source": [
    "For each of these tokens our model will have to learn an associated word embedding, and we don't want the embedding matrix to contain too many noisy words.\n",
    "\n",
    "Note the very time- and space-specific words, already embedded at a low level in our modeling approach. Such specific tokens by a BPE tokenizer can also be an indication that the target vocabulary size is too larfe or that the corpus contains idiosyncratic tokens.\n",
    "\n",
    "Let's train a fresh tokenizer and anayse its learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fd5143f-a7c9-4f78-ac64-e810fafe630a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4122eacdecf84321ad303ae23effc01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdfdb7d5f014653b87d472d290d6ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's select 1-2GB of data (about 100K documents)\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "length = 100000\n",
    "dataset_name = \"sergi24sanchez/codeparrot-train\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "def batch_iterator(batch_size=10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)['content'] for _ in range(batch_size)]\n",
    "\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=12500, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4ecff95-79ef-4b93-86a9-7c2d6e6e8f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "['  ', '    ', '   ', '\\n    ', 'se', 'in', 're', 'on', 'te', 'or', '\\n       ', 'st', 'de', '\\n   ', '        ', 'th', ' =', 'le', ' a', 'lf', 'ti', 'me', 'self']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print(len(new_tokenizer.vocab))\n",
    "print([f'{new_tokenizer.convert_tokens_to_string([t])}' for t,_ in tokens[257:280]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d940fa94-56d8-453a-b262-8e6657dd1b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tack', ' plus', 'í', 'TestResult', ' calc', 'authorize', 'El', 'Patch', 'passed', 'srs', 'xi', ' Twisted']\n"
     ]
    }
   ],
   "source": [
    "print([f\"{new_tokenizer.convert_tokens_to_string([t])}\" for t,_ in tokens[-12:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1465b128-3b0e-4e5f-a5e8-5ef801eeebe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġs', 'ay', '_', 'hello', '()', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\")', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 's', 'ay', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "# tokenize our simple example\n",
    "print(new_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4e8e0e6-c112-4a30-ac13-2ce774044f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 35 Python keywords.\n",
      "No, keyword 'await' is not in the vocabulary\n",
      "No, keyword 'finally' is not in the vocabulary\n",
      "No, keyword 'nonlocal' is not in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# check if all the python reserved keywords are in the vocab\n",
    "import keyword\n",
    "\n",
    "print(f\"There are in total {len(keyword.kwlist)} Python keywords.\")\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer.vocab:\n",
    "        print(f\"No, keyword '{keyw}' is not in the vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fb9af-b07e-4959-a974-9bf0aad155cf",
   "metadata": {},
   "source": [
    "Let's try building a larger vocab using a larger sample of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5a884b1-d7a8-4873-911a-6026f6c4a961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6b3df987ab4e4e9a6c13b610051702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = 200000\n",
    "new_tokenizer_larger = tokenizer.train_new_from_iterator(batch_iterator(), vocab_size=32768, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6861ca0d-28ad-48be-bea6-3402368656de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' hexlify', 'partitioner', ' Elements', 'innerHTML', '仓', 'SomeClass', 'tlvs', ' siblings', 'hicles', 'Survey', 'ManagementClient', ' 為']\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print([f'{new_tokenizer_larger.convert_tokens_to_string([t])}' for t,_ in tokens[-12:]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc96c87e-4210-4fa9-ab79-370f37c52195",
   "metadata": {},
   "source": [
    "It doesn't show any regular programming keywords here, which is promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "525c64cb-3624-4c3f-8d56-4e8346cf9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '()', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\")', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(new_tokenizer_larger(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b51caa-33fe-4087-a389-ab5fb40e225c",
   "metadata": {},
   "source": [
    "Indents are conveniently kept. This seems more in line with our expectations of the data the model may see in the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12293084-e4e7-4152-8139-893647ae1223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, keyword 'nonlocal' is not in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer_larger.vocab:\n",
    "        print(f\"No, keyword '{keyw}' is not in the vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188bdc0-ee2d-413f-af78-0df820880240",
   "metadata": {},
   "source": [
    "We'll measure the performance of the tokenizer basd on the model's performance.\n",
    "\n",
    "## Saving a Custom Tokenizer on the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2193b2bc-6294-46bf-ac5a-e3fdffa17bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/transformers/utils/hub.py:731: UserWarning: The `organization` argument is deprecated and will be removed in v5 of Transformers. Set your organization directly in the `repo_id` passed instead (`repo_id={organization}/{model_id}`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sergi24sanchez/codeparrot/commit/889d8522a8e499a08d4dd4a4999ce301a33b830c', commit_message='Upload tokenizer', commit_description='', oid='889d8522a8e499a08d4dd4a4999ce301a33b830c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/sergi24sanchez/codeparrot', endpoint='https://huggingface.co', repo_type='model', repo_id='sergi24sanchez/codeparrot'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = \"codeparrot\"\n",
    "org = \"sergi24sanchez\"\n",
    "new_tokenizer_larger.push_to_hub(model_ckpt, organization=org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52a475e0-3345-4d71-b846-831661f53c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78168b535bd64f9283ab0d9fa7f8d83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916ff9af33d5459cb84834d1acf82c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a19634fa19242d3b89b39430921d676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afdf9ebf3bd043808daeb65f25486fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adbcd27eb2fe4977be3854bcdd64fddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġsay', '_', 'hello', '()', 'ĊĠĠĠ', 'Ġprint', '(\"', 'Hello', ',', 'ĠWorld', '!\")', 'Ċ', 'Ċ', '#', 'ĠPrint', 'Ġit', 'Ċ', 'say', '_', 'hello', '()', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(org + '/' + model_ckpt)\n",
    "print(reloaded_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cda23844-7631-4f52-8bb6-fe8e716afbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/sergi24sanchez/codeparrot-small_vocabulary/commit/8f85c7fe02327913415846bed11a88381c4e852f', commit_message='Upload tokenizer', commit_description='', oid='8f85c7fe02327913415846bed11a88381c4e852f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/sergi24sanchez/codeparrot-small_vocabulary', endpoint='https://huggingface.co', repo_type='model', repo_id='sergi24sanchez/codeparrot-small_vocabulary'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.push_to_hub(org + '/' + model_ckpt + \"-small_vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d4d07-d606-4f26-8e94-cdf3b2b890e2",
   "metadata": {},
   "source": [
    "# Training a Model from Scratch\n",
    "\n",
    "Which architecture works best for the task, initialize a fresh model without pretrained weights, set up a custom data laoding class, and create a scalable training loop.\n",
    "\n",
    "*In this section a longer thatn usual script to train a model on a distributed infrastructure will be implemented. You should not run each code snippet independently, but instead download the script provided in the [Transformers repository](https://github.com/huggingface/transformers-research-projects/tree/main/codeparrot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d923d-972e-4477-ae60-703e5ccbc3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
