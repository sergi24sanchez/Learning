{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16f47a4e-2a96-49e3-9ffd-8c6039b38014",
   "metadata": {},
   "source": [
    "Comparison text generations from GPT and GPT-2 to illustrate the notion of a model being skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c1d6f10-78a5-4c7b-a497-74955e692ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/ssanchez/env/transformers/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb86ab156e54f70a39758ff7ff46128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97242be676894e0997d4827abca76652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2025-07-01T14:46:16.894739Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Reqwest(reqwest::Error { kind: Request, url: \\\"https://cas-server.xethub.hf.co/reconstruction/986657f8b77a4a690d73500aa22cbda25b96a55bd9b689e8312ebd983a7108d5\\\", source: hyper_util::client::legacy::Error(Connect, ConnectError(\\\"dns error\\\", Custom { kind: Uncategorized, error: \\\"failed to lookup address information: Name or service not known\\\" })) }). Retrying...\"},\"filename\":\"/home/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":200}\n",
      "{\"timestamp\":\"2025-07-01T14:46:16.894830Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Retry attempt #0. Sleeping 651.203428ms before the next attempt\"},\"filename\":\"/root/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/reqwest-retry-0.7.0/src/middleware.rs\",\"line_number\":171}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533b818b7e64432fa99426bf02161474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b30f58e0ffd4bee887a34445692bc99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa0e60b62aa4cceaea897505b7685b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be28df425204cf6b08c177c25bb9062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9c958da2264b1bac1f6430eed703c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generation_gpt = pipeline(\"text-generation\", model=\"openai-gpt\")\n",
    "generation_gpt2 = pipeline(\"text-generation\", model=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9918d9-ebe3-4350-8779-5e98a0472a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT size:116.5M parameters\n",
      "GPT2 size:124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "print(f\"GPT size:{model_size(generation_gpt.model)/1000**2:.1f}M parameters\")\n",
    "print(f\"GPT2 size:{model_size(generation_gpt2.model)/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a33f24-0706-4e38-b4d2-a6ff734693de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT completions:\n",
      "1.\n",
      "When they came back from the cemetery and said their good - byes at the gravesite. \n",
      " at first i was so happy to have them return to me, i couldn't contain myself. with the relief we could share and the sense of relief they\n",
      "2.\n",
      "When they came back into the room. they were smiling at me and i thought i saw a hint of relief. we were all exhausted as we headed down the hallway and into the living room. \n",
      " \" where have you been? we've been so\n",
      "3.\n",
      "When they came back out to the clearing she was with, but they didn't see her. when she stopped by the river to drink from the spring, she saw a strange object floating across toward her. then, as suddenly as it had appeared,\n",
      "\n",
      "GPT2 completions:\n",
      "1.\n",
      "When they came back, there was a small group of us waiting for them there as our security guard stood by them, then they began to run, kicking, slashing and trying to frighten me into silence. It took about ten minutes before they\n",
      "2.\n",
      "When they came back, the car was already moving.\n",
      "\n",
      "\"There was a guy who was wearing a hooded sweatshirt that was holding a stick at the foot of his chest,\" says a relative who knew the man, calling himself Robert\n",
      "3.\n",
      "When they came back for it, they decided not to try it out because you need to be careful with the weight (and you will burn lots of fuel).\n",
      "\n",
      "\n",
      "We are now back in business!! We think of the next thing we need\n"
     ]
    }
   ],
   "source": [
    "def enum_pipeline_outputs(pipe, prompt, num_return_sequences):\n",
    "    out = pipe(prompt, num_return_sequences=num_return_sequences, clean_up_tokenization_spaces=True)\n",
    "    return \"\\n\".join(f\"{i+1}.\" + s[\"generated_text\"] for i, s in enumerate(out))\n",
    "\n",
    "prompt = \"\\nWhen they came back\"\n",
    "print(\"GPT completions:\\n\" + enum_pipeline_outputs(generation_gpt, prompt, 3))\n",
    "print(\"\")\n",
    "print(\"GPT2 completions:\\n\" + enum_pipeline_outputs(generation_gpt2, prompt, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d4449-a5a1-4640-b70e-5910a195b10c",
   "metadata": {},
   "source": [
    "## Building a Custom Code Dataset\n",
    "### Creating a dataset with Google BigQuery\n",
    "\n",
    "tota la perafernalia que he hagut de fer per executar una consulta, guardar els resultats en una taula d'un dataset i passar les dades a una carpeta del bucket 'npl_transformers'.\n",
    "Després he instalat el [gcloud CLI](https://cloud.google.com/sdk/docs/install) al servidor.\n",
    "Llavors he pogut llançar la comana: ``gsutil -m -o \"GSUtil:parellel_process_count=1\" cp -r gs:<nom_del_bucket>/<carpeta>/resultados-*.json.gz /datasets/codeparrot``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7403ba6-471b-4174-9674-929dcee53163",
   "metadata": {},
   "source": [
    "## Working with Large Datasets\n",
    "\n",
    "Loading a very large dataset is often a challenging task, in particular when the data is larger than your machine's RAM.\n",
    "In our example, we have 50 GB of compressed data and about 200 GB of uncompressed data.\n",
    "\n",
    "Thankfully Datasets has two specific features that allow you to set yourself free from RAM and hard drive space limitations: **memory mapping** and **streaming**.\n",
    "\n",
    "### Memory mapping\n",
    "\n",
    "Overcoming RAM limitations --> uses a mechanism of zero-copy and zero-overhead memory mapping.\n",
    "Basically, each dataset is cached on the drive in a file that is a direct reflection of the content in RAM memory. Instead of loading the dataset in RAM, it opens a read-only pointer to this file and uses it as a substitue for RAM (using hard drive as an extension of RAM).\n",
    "\n",
    "Here, we will direcly load our 50GB of compressed JSON files that we have stored locally in the *codeparrot* repository.\n",
    "Decompress JSON files. Be careful, cause this uses **180GB of free disk!!!**\n",
    "\n",
    "*delete_extracted=True* in the dataset's downloading configuration, we can make sure that we delete all the files we don't need anymore as soon as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3cf18bd-102f-4341-af46-cc2190a65d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf4fe40d09c4d3d9e7f24d261c7e8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a2c79b4abb469db2f0b77536aa196a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/500 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2370ad6485a54ccfb34c8a6ab90c60fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062f580d205b4fe283a501090776c5f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DownloadConfig, Features, Value\n",
    "import os\n",
    "\n",
    "download_config = DownloadConfig(delete_extracted=True)\n",
    "dataset_path = os.path.expanduser(\"~/datasets/codeparrot/export_results/*.json.gz\")\n",
    "\n",
    "features = Features({\n",
    "    \"repo_name\": Value(\"string\"),\n",
    "    \"path\": Value(\"string\"),\n",
    "    \"copies\": Value(\"string\"),\n",
    "    \"size\": Value(\"string\"),\n",
    "    \"content\": Value(\"string\"),\n",
    "    \"license\": Value(\"string\")\n",
    "})\n",
    "\n",
    "dataset = load_dataset(\"json\",\n",
    "                       data_files=dataset_path, split=\"train\",\n",
    "                      download_config=download_config, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4e59bb-79f9-40e0-aa84-8b5bfc6f837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gunzip -c ~/datasets/codeparrot/export_results/resultados-000000000000.json.gz > ./uno.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd876c86-9b6f-4b32-adca-ad959d6110c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d1f510e39e429bac1007878423b674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
      "    num_rows: 38184\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#from datasets import load_dataset\n",
    "#\n",
    "#dataset = load_dataset(\"json\", data_files=\"./uno.json\", split=\"train\")\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94616d64-217a-44d7-8bf9-2861e7a77d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of python files code in dataset : 18923569\n",
      "Dataset size (cache file) : 97.06 GB\n",
      "RAM used: 1720 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"Number of python files code in dataset : {len(dataset)}\")\n",
    "ds_size = sum(os.stat(f[\"filename\"]).st_size for f in dataset.cache_files)\n",
    "# os.stat.st_size is expressed in bytes, so we convert to GB\n",
    "print(f\"Dataset size (cache file) : {ds_size / 2**30:.2f} GB\")\n",
    "# Process.memory_ingo is expressed in bytes, so we convert to MB\n",
    "print(f\"RAM used: {psutil.Process(os.getpid()).memory_info().rss >> 20} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b19efb-3b4b-4223-9baa-5d80d233e32f",
   "metadata": {},
   "source": [
    "What if you can't free enough disk space to store the full dataset locally?\n",
    "\n",
    "## Streaming\n",
    "\n",
    "An alternative to scaling up the server you are using is to *stream* the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1894aab4-4113-44ec-89b4-24dcd77274ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa73b0bbc5a440a497246a3f5d008b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "streamed_dataset = load_dataset(\"json\",\n",
    "                       data_files=dataset_path, split=\"train\",\n",
    "                      download_config=download_config, features=features,\n",
    "                               streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9d223-0de4-4bd2-bbf2-a7ee7e495188",
   "metadata": {},
   "source": [
    "In streaming mode, the compressed JSON files will be opened and read on the fly. Out dataset is now an *IterableDataset* object. This means that we cannot access random elements of it (streamed_dataset[1264]), but we need to read it in order, for instance with ``next(iter(streamed_dataset))``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c44e734d-04e7-4d90-b579-cc323fe65daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(streamed_dataset)\n",
    "\n",
    "print(dataset[0] == next(iterator))\n",
    "print(dataset[1] == next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed7e7a-81c1-4408-91e1-eba7e74fb36e",
   "metadata": {},
   "source": [
    "The original raw files are extracted and read on the fly when a new batch of examples is requested, and onñy that batch is loaded in memory.\n",
    "\n",
    "One step further-- instead of pointing to the local dataset we can reference the dataset on the Hub, and then directly download samples without downloading the raw files locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0145c66d-df0c-40bb-8adc-197e1a71126a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9a3927804842c2bcd0a342d438bc88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba046b8708a47cea9c872f3e28f2e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "remote_dataset = load_dataset('transformersbook/codeparrot', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f38625-c75e-4387-88c7-d0449548fc55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
