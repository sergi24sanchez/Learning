{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776444df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cee051f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1735fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62d58df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182580, 3]) torch.Size([182580])\n",
      "torch.Size([22767, 3]) torch.Size([22767])\n",
      "torch.Size([22799, 3]) torch.Size([22799])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e181cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8674868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1 # if variables are 0 it can mask a correct implementation of the gradient. When everything is 0 it simplifies and gets a much simpler expression of the gradient.\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5de51a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66f5634e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5025, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv  # implicit broadcast\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ce60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = -(a + b + c) / 3\n",
    "# dloss / d? = -1/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1acfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.shape, counts_sum_inv.shape\n",
    "# c = a * b\n",
    "# a[3x3] * b[3x1] --> c[3x3]\n",
    "# a11*b1 a12*b1 a13*b1  b1 feeding into various branches --> if a node is used multiple times the gradients of all of its uses sum\n",
    "# a21*b2 a22*b2 a23*b2\n",
    "# a31*b3 a32*b3 a33*b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a11 a12 a13 -- > b1 (=a11 + a12 + a13)   (1 + 1 + 1)\n",
    "# a21 a22 a23 -- > b2 (=a21 + a22 + a23)   the derivative of b2 is flowing horizontally to all of the components\n",
    "# a31 a32 a33 -- > b3 (=a31 + a32 + a33)\n",
    "# addition is a router of gradient. Whatever gradient that comes from above, it just gets routed equally to all the elements that participate\n",
    "counts.shape, counts_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c11 c12 c13 = a11 a12 a13       b1 --> it gets repeated the same number of columns of matrix C and A\n",
    "# c21 c22 c23 = a21 a22 a23   -   b2\n",
    "# c31 c32 c33 = a31 a32 a33       b3\n",
    "# c11 = a11 - b1 1\n",
    "# c12 = a12 - b1 1\n",
    "# c13 = a13 - b1 1\n",
    "# c21 = a21 - b2 2\n",
    "# c22 = a22 - b2 2\n",
    "# c23 = a23 - b2 2\n",
    "# c31 = a31 - b3 3\n",
    "# c32 = a32 - b3 3\n",
    "# c33 = a33 - b3 3   b's are broadcast so we have to do the additional sum\n",
    "norm_logits.shape, logits.shape, logit_maxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bcfec8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11fb30f70>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7eeb71a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([27, 10]), torch.Size([32, 3, 10]), torch.Size([32, 3]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# emb = C[Xb] # embed the characters into vectors\n",
    "# embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "C.shape, emb.shape, Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9b22f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually,\n",
    "# backpropagating through exactly all of the variables\n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n),Yb] = -1.0/n\n",
    "dprobs = dlogprobs * (1.0 / probs)\n",
    "# if counts_sum_inv was the replicated version dcounts_sum_inv = (counts * dprobs)\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True) # sum across horizontal direction and retain counts_sum_inv dimensions\n",
    "dcounts = counts_sum_inv * dprobs # 1st branch\n",
    "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum # 2nd branch\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True) # I DONT UNDERSTAND YET WHY THE SUM\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = -0.5 * (bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar # the broadcasting is doing the replication\n",
    "dbndiff += (2 * bndiff) * dbndiff2\n",
    "dbnmeani = (-dbndiff).sum(0, keepdim=True)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += (1/n) * torch.ones_like(hprebn) * dbnmeani\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix, :] += demb[k,j] # the same row could have been used many times\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe15b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a54e929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1) # do softmax along the rows\n",
    "dlogits[range(n), Yb] -= 1 # subtract 1 at the correct position(label) for each example\n",
    "dlogits /= n # average of all the losses, gradient needs to be scaled\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd757589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b4f1e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0378, 0.0166, 0.0274, 0.0253, 0.0426, 0.0613, 0.0804, 0.0209, 0.0373,\n",
       "        0.0364, 0.0314, 0.0303, 0.0433, 0.0595, 0.0591, 0.0626, 0.0553, 0.0178,\n",
       "        0.0243, 0.0292, 0.0399, 0.0124, 0.0214, 0.0277, 0.0324, 0.0286, 0.0388],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "944d436f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0378,  0.0166,  0.0274,  0.0253,  0.0426,  0.0613,  0.0804,  0.0209,\n",
       "         0.0373,  0.0364,  0.0314,  0.0303,  0.0433,  0.0595,  0.0591,  0.0626,\n",
       "         0.0553,  0.0178, -0.9757,  0.0292,  0.0399,  0.0124,  0.0214,  0.0277,\n",
       "         0.0324,  0.0286,  0.0388], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5c2413f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-9.3132e-10, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2a03715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12265e5f0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "296118a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e6c47f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64]),\n",
       " torch.Size([32, 64]),\n",
       " torch.Size([64]),\n",
       " torch.Size([32, 64]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hprebn.shape, hpreact.shape, hpreact.sum(0).shape, bnraw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3fd297b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0, keepdim=True) - (n/(n-1) * bnraw * (dhpreact * bnraw).sum(0, keepdim=True)))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dcae339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.6603\n",
      "  10000/ 200000: 2.4524\n",
      "  20000/ 200000: 2.3466\n",
      "  30000/ 200000: 2.0812\n",
      "  40000/ 200000: 2.0424\n",
      "  50000/ 200000: 2.4629\n",
      "  60000/ 200000: 2.3185\n",
      "  70000/ 200000: 2.0608\n",
      "  80000/ 200000: 1.9473\n",
      "  90000/ 200000: 2.0369\n",
      " 100000/ 200000: 2.4279\n",
      " 110000/ 200000: 2.2282\n",
      " 120000/ 200000: 2.1344\n",
      " 130000/ 200000: 2.4497\n",
      " 140000/ 200000: 2.2481\n",
      " 150000/ 200000: 2.3993\n",
      " 160000/ 200000: 2.2185\n",
      " 170000/ 200000: 1.9795\n",
      " 180000/ 200000: 2.3408\n",
      " 190000/ 200000: 1.9266\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "# kick off optimization\n",
    "for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, 1) # do softmax along the rows\n",
    "    dlogits[range(n), Yb] -= 1 # subtract 1 at the correct position(label) for each example\n",
    "    dlogits /= n # average of all the losses, gradient needs to be scaled\n",
    "    # 2nd layer backprop\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    dh = dlogits @ W2.T\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0, keepdim=True) - (n/(n-1) * bnraw * (dhpreact * bnraw).sum(0, keepdim=True)))\n",
    "    # 1st layer backprop\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix, :] += demb[k,j] # the same row could have been used many times\n",
    "    \n",
    "    #dc, dW1, db1, dW2, db2, dbngain, dbnbias = None, None, None, None, None, None, None\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    \n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "        #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "        p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    #if i >= 1000: # TODO: delete early breaking when you're ready to train the full net\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "481c301a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, Tensor out)\n * (Tensor input, name dim, bool keepdim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# useful for checking your gradients\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p,g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parameters, grads):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mcmp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mcmp\u001b[0;34m(s, dt, t)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcmp\u001b[39m(s, dt, t):\n\u001b[0;32m----> 3\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      4\u001b[0m     app \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mallclose(dt, t\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m      5\u001b[0m     maxdiff \u001b[38;5;241m=\u001b[39m (dt \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: all() received an invalid combination of arguments - got (bool), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, tuple of ints dim, bool keepdim, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, Tensor out)\n * (Tensor input, name dim, bool keepdim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "for p,g in zip(parameters, grads):\n",
    "    cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d1d1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5baea0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.072636127471924\n",
      "val 2.1128201484680176\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x] # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac650129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch results\n",
    "#train 2.0686187744140625\n",
    "#val 2.1115527153015137\n",
    "# Manual backprop results\n",
    "#train 2.072636127471924\n",
    "#val 2.1128201484680176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9d6cfd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mora.\n",
      "mayanna.\n",
      "elvedhayla.\n",
      "remmadiendraegan.\n",
      "chedieliah.\n",
      "milopaleigh.\n",
      "van.\n",
      "nar.\n",
      "katella.\n",
      "kalin.\n",
      "shubergshimiel.\n",
      "kindreelynn.\n",
      "novana.\n",
      "ura.\n",
      "geder.\n",
      "yarulyehsyn.\n",
      "ajaysen.\n",
      "daihaan.\n",
      "alynn.\n",
      "juluna.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        # ------------\n",
    "        # forward pass:\n",
    "        # Embedding\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "        embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "        logits = h @ W2 + b2 # (N, vocab_size)\n",
    "        # ------------\n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch examples\n",
    "#mora.\n",
    "#mayannieel.\n",
    "#nduryal.\n",
    "#remman.\n",
    "#endraege.\n",
    "#zered.\n",
    "#eliah.\n",
    "#milopaleigh.\n",
    "#sananarielle.\n",
    "#malaia.\n",
    "#noshubergihimie.\n",
    "#trick.\n",
    "#welle.\n",
    "#jose.\n",
    "#casund.\n",
    "#geder.\n",
    "#yarleyeks.\n",
    "#kaysh.\n",
    "#samyah.\n",
    "#hal.\n",
    "# Manual backprop examples\n",
    "mora.\n",
    "mayanna.\n",
    "elvedhayla.\n",
    "remmadiendraegan.\n",
    "chedieliah.\n",
    "milopaleigh.\n",
    "van.\n",
    "nar.\n",
    "katella.\n",
    "kalin.\n",
    "shubergshimiel.\n",
    "kindreelynn.\n",
    "novana.\n",
    "ura.\n",
    "geder.\n",
    "yarulyehsyn.\n",
    "ajaysen.\n",
    "daihaan.\n",
    "alynn.\n",
    "juluna."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
