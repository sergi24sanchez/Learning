# Learning [Deep Learning]
Welcome! 
This repository documents my complete [deep] learning journey.

---

## ğŸš€ My Goals & Motivation

âœ… Build real-world skills with state-of-the-art NLP  
âœ… Understand transformer architectures beyond black-box usage  
âœ… Learn practical workflows with ğŸ¤— Transformers and Datasets  
âœ… Develop a project portfolio to demonstrate deep learning capabilities

---

# ğŸ› ï¸ Projects

## ğŸ¤– Natural Language Processing with Transformers

[![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)](https://www.python.org/)
[![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97-HuggingFace-yellow?logo=huggingface)](https://huggingface.co/)

> ğŸ“˜ A hands-on implementation of state-of-the-art NLP using **Transformers**, based on the book by Tunstall, von Werra & Wolf.  
> ğŸ›  Built with ğŸ¤— Transformers, PyTorch, ONNX Runtime, and more.

---

### ğŸ“Œ Overview

Over several weeks, I explored modern NLP through full-stack implementations â€” from fine-tuning BERT to training a GPT-style model from scratch â€” applying lessons to real-world tasks such as:

- ğŸŒŸ **Text Classification** (Sentiment, NER)
- ğŸŒ **Multilingual Transfer Learning** (XLM-R, zero-shot NER)
- âœï¸ **Text Generation & Summarization** (T5, BART, PEGASUS)
- â“ **Question Answering Systems** (DPR + FAISS + RAG)
- âš¡ **Transformer Optimization** (Quantization, Distillation, ONNX)
- ğŸ§  **Training from Scratch** (Custom GPT-2 for code autocompletion)
- ğŸ”® **Emerging Architectures** (Sparse Attention, ViT, CLIP, DALLÂ·E)

---

### ğŸ“Š Sample Outputs


> ğŸ§¾ **Zero-Shot & Few-Shot Learning Comparison** (Pre-trained vs Data Augmentation vs Fine-tuning)
>
> <img src="NLPTransformers/img/few-shot_learning.png" width="500"/>

> ğŸ“ˆ **Model Efficiency Optimization**
>
> <img src="NLPTransformers/img/transformers_efficiency.png" width="500"/>

> ğŸ” **117M parameter GPT-2 Python Code Autocompletion Model Training**
>
> <img src="NLPTransformers/img/train_loss.png" width="500"/>

---

### ğŸ§  Key Results & Learnings

- âœ… Fine-tuned multiple models for classification, generation, and QA
- ğŸ§© Used FAISS & DPR for scalable document retrieval
- âš™ï¸ Built an efficient multi-GPU pipeline with `ğŸ¤— Accelerate`
- ğŸ§ª Compared summarization quality using BLEU, ROUGE, and ground-truth
- ğŸ”§ Applied optimization techniques to reduce latency in production

---

### ğŸ”— Tools & Libraries Used

- [ğŸ¤— Transformers](https://github.com/huggingface/transformers)
- [ğŸ¤— Datasets](https://github.com/huggingface/datasets)
- [ğŸ¤— Accelerate](https://github.com/huggingface/accelerate)
- [PyTorch](https://pytorch.org/)
- [scikit-learn](https://scikit-learn.org/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [ONNX](https://onnx.ai/)
- [ONNX Runtime](https://onnxruntime.ai/)
- [Weights & Biases](https://wandb.ai/) *(optional for experiment tracking)*


---

## ğŸ Python Course

## ğŸ³ [Andrew Karpathy](https://karpathy.ai/)'s NN Zero-to-Hero

---

# ğŸ§‘â€ğŸ’» About Me

**Sergi SÃ¡nchez Bonilla**  
Machine Learning Practitioner | NLP Explorer | Lifelong Learner

ğŸ“« [LinkedIn](https://www.linkedin.com/in/sergi-sanchez-bonilla)  
